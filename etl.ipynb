{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e61f878f-2375-4ca3-a949-12c37371286c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T04:18:40.051934Z",
     "iopub.status.busy": "2022-01-28T04:18:40.051638Z",
     "iopub.status.idle": "2022-01-28T04:18:40.160583Z",
     "shell.execute_reply": "2022-01-28T04:18:40.159585Z",
     "shell.execute_reply.started": "2022-01-28T04:18:40.051899Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f6bbb34b6c435ca7f9cfbbd895ec02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions  as F \n",
    "# udf, col, year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read('dl.cfg')\n",
    "\n",
    "# #os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "# #os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# def create_spark_session():\n",
    "#     spark = SparkSession \\\n",
    "#         .builder \\\n",
    "#         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "#         .getOrCreate()\n",
    "#     return spark\n",
    "\n",
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Processes the songs data files and extracts info from it.\n",
    "    :param spark: a spark session instance\n",
    "    :param input_data: input file path\n",
    "    :param output_data: output file path\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + \"song_data/*/*/*/*\"\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data, mode='PERMISSIVE', \\\n",
    "                         columnNameOfCorruptRecord='corrupt_record')\\\n",
    "                .drop_duplicates()\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(\"song_id\",\"title\",\"artist_id\",\"year\",\"duration\").distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.parquet\\\n",
    "    (output_data + \"songs/\", mode=\"overwrite\", partitionBy=[\"year\",\"artist_id\"])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(\"artist_id\",\"artist_name\",\"artist_location\",\"artist_latitude\",\"artist_longitude\")\\\n",
    "    .distinct()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(output_data + \"artists/\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Processes the event log file and extracts info from it.\n",
    "    :param spark: a spark session instance\n",
    "    :param input_data: input file path\n",
    "    :param output_data: output file path\n",
    "    \"\"\"\n",
    "    log_data = os.path.join(input_data, \"log_data/\")\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data, mode='PERMISSIVE', \\\n",
    "                         columnNameOfCorruptRecord='corrupt_record').distinct()\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(F.col('page') == \"NextSong\")\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select(\"userId\",\"firstName\",\"lastName\",\"gender\",\"level\").drop_duplicates()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet(os.path.join(output_data, \"users/\") , mode=\"overwrite\")\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = F.udf(lambda x : datetime.utcfromtimestamp(int(x)/1000), TimestampType())\n",
    "    df = df.withColumn(\"start_time\", get_timestamp(\"ts\"))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table =  df.withColumn(\"hour\",F.hour(\"start_time\"))\\\n",
    "                    .withColumn(\"day\",F.dayofmonth(\"start_time\"))\\\n",
    "                    .withColumn(\"week\",F.weekofyear(\"start_time\"))\\\n",
    "                    .withColumn(\"month\",F.month(\"start_time\"))\\\n",
    "                    .withColumn(\"year\",F.year(\"start_time\"))\\\n",
    "                    .withColumn(\"weekday\",F.dayofweek(\"start_time\"))\\\n",
    "                    .select(\"ts\",\"start_time\",\"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\").distinct()\n",
    "\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.parquet\\\n",
    "    (os.path.join(output_data, \"time_table/\"), mode='overwrite', partitionBy=[\"year\",\"month\"])\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read\\\n",
    "                .format(\"parquet\")\\\n",
    "                .option(\"basePath\", os.path.join(output_data, \"songs/\"))\\\n",
    "                .load(os.path.join(output_data, \"songs/*/*/\"))\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = df.join(song_df, df.song == song_df.title, how='inner')\\\n",
    "                        .select(F.monotonically_increasing_id().alias(\"songplay_id\")\\\n",
    "                            ,F.col(\"start_time\"),F.col(\"userId\").alias(\"user_id\"),\"level\",\"song_id\",\"artist_id\", \n",
    "                            F.col(\"sessionId\").alias(\"session_id\"), \"location\", F.col(\"userAgent\").alias(\"user_agent\"))\\\n",
    "                        .join(time_table, ['start_time'], 'inner')\\\n",
    "                         .select(\"songplay_id\", 'start_time', \"user_id\", \"level\", \"song_id\", \"artist_id\", \"session_id\", \n",
    "                                 \"location\", \"user_agent\", \"year\", \"month\")\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.drop_duplicates().write.parquet(os.path.join(output_data, \"songplays/\"), mode=\"overwrite\", partitionBy=[\"year\",\"month\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f54140b-f6d7-4086-be6c-de751073492c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T04:18:40.570294Z",
     "iopub.status.busy": "2022-01-28T04:18:40.569988Z",
     "iopub.status.idle": "2022-01-28T04:18:40.664792Z",
     "shell.execute_reply": "2022-01-28T04:18:40.663738Z",
     "shell.execute_reply.started": "2022-01-28T04:18:40.570258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aff7affd7704b57b62837747a2407fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3://patrickudacityde/\"\n",
    "    output_data = \"s3://patrickudacityde/output/\"\n",
    "\n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931f2a18-a86a-4550-9f08-d87dc49f4c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T04:18:41.708273Z",
     "iopub.status.busy": "2022-01-28T04:18:41.707968Z",
     "iopub.status.idle": "2022-01-28T04:19:02.075796Z",
     "shell.execute_reply": "2022-01-28T04:19:02.069169Z",
     "shell.execute_reply.started": "2022-01-28T04:18:41.708237Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708846d16ac2446586deb8628d8f128e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_data = \"s3://patrickudacityde/\"\n",
    "output_data = \"s3://patrickudacityde/output/\"\n",
    "\n",
    "# process_song_data(spark, input_data, output_data)    \n",
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ce04d40-da57-42fd-9520-d6ed33780afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T04:23:43.846969Z",
     "iopub.status.busy": "2022-01-28T04:23:43.846458Z",
     "iopub.status.idle": "2022-01-28T04:23:53.247189Z",
     "shell.execute_reply": "2022-01-28T04:23:53.245886Z",
     "shell.execute_reply.started": "2022-01-28T04:23:43.846909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6da1d893ac4907809502de359f5741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songplays_df = spark.read\\\n",
    "        .format(\"parquet\")\\\n",
    "        .option(\"basePath\", os.path.join(output_data, \"songplays/\"))\\\n",
    "        .load(os.path.join(output_data, \"songplays/*/*/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a16676b-50a1-4624-8a70-24315727bd88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T04:24:03.067163Z",
     "iopub.status.busy": "2022-01-28T04:24:03.066852Z",
     "iopub.status.idle": "2022-01-28T04:24:05.415744Z",
     "shell.execute_reply": "2022-01-28T04:24:05.414442Z",
     "shell.execute_reply.started": "2022-01-28T04:24:03.067128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0c64d65a70493cad503fdc075fad07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "| songplay_id|          start_time|user_id|level|           song_id|         artist_id|session_id|            location|          user_agent|year|month|\n",
      "+------------+--------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+----+-----+\n",
      "|           0|2018-11-21 21:56:...|     15| paid|SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|       818|Chicago-Napervill...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "| 17179869184|2018-11-14 05:06:...|     10| free|SOGDBUF12A8C140FAA|AR558FS1187FB45658|       484|Washington-Arling...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "| 17179869185|2018-11-27 22:35:...|     80| paid|SOGDBUF12A8C140FAA|AR558FS1187FB45658|       992|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|188978561024|2018-11-19 09:14:...|     24| paid|SOGDBUF12A8C140FAA|AR558FS1187FB45658|       672|Lake Havasu City-...|\"Mozilla/5.0 (Win...|2018|   11|\n",
      "+------------+--------------------+-------+-----+------------------+------------------+----------+--------------------+--------------------+----+-----+"
     ]
    }
   ],
   "source": [
    "songplays_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835497e-a0ed-434b-83fd-a34506b34f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
